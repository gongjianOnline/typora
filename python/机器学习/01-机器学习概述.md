# 人工智能概述

## 1. 人工智能三大概述

### 1.1 人工智能 AI / 机器学习 ML / 深度学习 DL 三者关系

机器学习是实现人工智能的一种途径,深度学习是机器学习的一种方法发展而来

目前的人工智能技术体系

- 基于统计学的传统机器学习方法
- 基于神经网络的深度学习方法

### 1.2 样本 / 特征 / 标签

- 样本( sample )
  - 一行数据就是一个样本; 多个样本组成数据集; 有时一条样本也叫做一条记录
- 特征 ( feature )
  - 一列数据一个特征, 有时也叫做属性
- 标签 / 目标 ( label / target )
  - 模型要预测的那一列
- 特征如何理解(重点)
  - 特征就从数据中抽取出来的,对结果预测有用的信息, 如: 房价预测  车图片识别等

### 1.3 数据集划分

数据集可划分为两部分: 训练集   测试集  比例 8:2 ( 常用 )  / 7:3

- 训练集 (tarining set) : 用来训练模型 (model) 数据集
- 测试集 (testing set) : 用来测试模型的数据集

---

## 2. 机器学习算法分类

- 监督性学习  supervised Learning

  - 输入数据是由输入特征值和目标值所组成,即输入的训练数据有标签的

  - 数据集: 需要表述数据的标签 / 目标值

  - 有监督分类问题 & 回归问题

    - 分类问题
      - 目标值(标签值) 是不连续的
      - 分类种类: 二分类 多分类
    - 回归问题
      - 目标值(标签值)是连续的
- 无监督性学习 unSuperised Learning
  - 自己确认分类后算法自动根据特征对数据进行分类
  - 输入没有被标记,即样本的数据类别未知,没有标签,根据样本件的相似性,对样本聚集类,以发现事务内部结构及相互关系
  
- 半监督学习  Semi Supervised Learning

  - 工作原理
    - 让专家标注少量数据,利用已经标注的数据( 也就是带有类标签 ) 训练出一个模型
    - 再利用模型去套用未标记的数据
    - 通过询问领域专家分类结果与模型分类结果对比,从而对模型做进一步改善和提高
  -   优点: 半监督学习方式可大幅度降低标注成本,常用语数据标注场景
- 强化学习 Reinforcement Learning
  - 强化学习: 机器学习的一个重要分支
  - 应用场景: AlphaGo 各类游戏 对抗比赛  无人驾驶等
  - 基本原理:
    - 通过构建四个元素 Agent , 环境状态(State) , 行动(Action) , 奖励(Reward)
    - Agent 根据环境状态进行行动获得最多的累计奖励

  - 举例: 小孩学走路
    - 小孩 (Agent) 试图通过**采取行动**(行走),在**操作环境**(行走的表现)
    - 并且从一个状态转变到另一个状态(即他走的每一步)
    - 当他完成任务的子任务(即走了几步)时, 孩子得到**奖励(**巧克力)
    - 并且当他不能走路时,就不会给巧克力


---

##   3. 线性回归--模型训练推理

```python
# 导包  LinearRegression 为线性回归模型
from sklearn.linear_model import LinearRegression
import joblib

# 创建模型训练 & 保存模型
def create_demo01():
    # 准备数据 平时成绩 期末成绩 最终成绩
    x = [[80,86],[82,80],[85,78],[90,90],[86,82],[82,90],[78,80],[92,94]]
    y = [84.2,80.6,80.1,90,83.2,87.6,79.4,93.4]
    
    # 实例化 线性回归模型
    regression = LinearRegression()
    print("实例化线性回归模型",regression)
    
    # 模型训练
    # 打印线性回归模型参数 coef_  intercept_
    regression.fit(x,y)
    print("regression.coef[斜率]--->",regression.coef_)
    print("regression.intercept[偏置]--->",regression.intercept_)
	
    # 模型推理
    # mypered = regression.predict([[90,80]])
    # print('mypered',mypered )
    
    # 模型保存
    joblib.dump(regression,'./module/create_demo01.bin')

# 调用保存好的模型    
def handle_module():
    # 加载模型
    regression2 = joblib.load("./module/craete_demo01.bin")
    mypred2 = regression2.predict([[90,80]])
    print("斜率",mypred2.coef_)
    print("偏置",mypred2.intercept_)
    print("推理结果",myred2)
    
if __name__ = "__main__":
    # 调用模型训练函数
    create_demo01()
    # 调用模型推理函数
    handle_module()
```

## 4. KNN--模型训练推理

```python
# 导包
from slearn.neignbors import KNeighborsClassifier

def create_demo2():
   	# 搞笑镜头  拥抱镜头  打斗镜头 
    x = [[39,0,31],[3,2,65],[2,3,55],[9,38,2],[8,34,17],[5,2,57],[21,17,5],[45,2,9]]
    # 喜剧片 动作片 爱情片
    y = [0,1,2,2,2,1,0,0]
    
    # 实例化 KNN 模型
    estimator = KNeighborsClassifier(n_neighbors=3)
    print("estimator knn",estimator)
    
    # 模型训练
    estimator.fit(x,y)
    
    # 模型推理
    mypre = estimator.predict([[23,3,17]])
    print("推理结果",mypre)
    
if __name__ == "__main__":
    create_demo2() // [0] 喜剧片
```

| 部分                   | 含义                                                         |
| ---------------------- | ------------------------------------------------------------ |
| `KNeighborsClassifier` | **K 近邻分类器**（K-Nearest Neighbors Classifier）           |
| `n_neighbors=3`        | **只找离新样本最近的 3 个训练点**，然后“投票”决定新样本的类别 |

### 工作流程（极简版）

1. 训练时：它只是**把数据存起来**，不学参数。
2. 预测时：
   - 算新样本到所有训练点的距离（默认欧氏距离）。
   - 找出最近的 3 个邻居。
   - 这 3 个邻居里哪种标签多，新样本就判为哪一类。

总结:

`KNeighborsClassifier(n_neighbors=3)` 就是 **“找 3 个最近邻居投票决定类别”** 的懒人分类器。

---

## 5. KMeans 聚类 demo

属于无监督学习

```python
# 导报
# 导入 Kmeans 聚类算法
# 用途： 用来把数据分成若干组（簇），属于，无监督学习
# 核心思想：自动找出数据中相似的样本，把他们聚在一起
import os
os.environ["OMP_NUM_THREADS"] = "4"
from sklearn.cluster import KMeans
# 生成模拟的聚类样本数据
# 用途：常用测试聚类算法
# 功能：可以制定生成多少个样本点，簇的数量，分布范围
from sklearn.datasets import make_blobs
# 导入可视化图库
import matplotlib.pyplot as plt
# 导入 Calinski-Harabasz 指数（CH指数），一种聚类效果评估指标
# 用途：衡量聚类结果的好坏（数值越大越好）
# 原理：比较簇间离散度和簇内紧密度的比值
from sklearn.metrics import calinski_harabasz_score


def dm05_kmeans():
    """
    	随机数据点，n_samples为样本个数为 1000 条，n_features为每个样本有两个特征（平面散点图的x，y）
    	centers表示数据的原点 [[-1,-1],[0,0],[1,1],[2,2]] 分布为四个点
    	cluster_std 每个点的离散度
    	random_state 持久化让每次运行都保持数据唯一
    """
    x,y = make_blobs(n_samples=1000,n_features=2,centers=[[-1,-1],[0,0],[1,1],[2,2]],
                     cluster_std=[0.4,0.2,0.2,0.2],random_state=1)
    plt.figure()
    plt.scatter(x[:,0],x[:,1],marker='o')
    plt.show()
    
    # 模型实例化
    kmeans_cls = KMeans(n_clusters=4,init="k-means++",n_init="auto")
    # 模型预测
    y_pred = kmeans_cls.fit_predict(x)
    plt.scatter(x[:,0],x[:,1],c=y_pred)
    plt.show()
    # 模型评估
    print(calinski_harabasz_score(x,y))
    # 模型评估 模型评估的值越大越好，有时会超过原始的评估值
    print("模型评估---->",calinski_harabasz_score(x, y_pred))
    
if __name__ = "__main__":
    dm05_kmeans()
    
```

## 6. 特征工程

概述：

- 特征 Feature ： 对任务有用的属性信息
- 特征工程： 利用专业背景知识和技巧处理数据，让模型效果更好

特征工程的内容

- 特征提取 Feature extraction：特征向量
- 特征预处理 Feature preprocessing ：不同特征对模型影响的一致性
- 特征降维 Feature decomposition: 保证数据的主要信息要保留下来
- 特征选择 Feature selection：从特征中选择出一些重要的特征训练模型
- 特征组合 Feature Crosses：把多个特征合并组合成一个特征

## 7. 拟合

- 拟合 fitting 用在机器学习领域，用来表示模型对样本的拟合情况
- 欠拟合 under-fitting 模型在训练集上表现很差、在测试集上也很差
- 过拟合 over-fitting 模型在训练集上表现很好、在测试集上表现很差

![](https://raw.githubusercontent.com/gongjianOnline/ImgHosting/main/img/8e9cf4ea-b62b-4a18-bc47-d05353dfcdb2.png)

- 欠拟合产生的原因：模型过于简单
- 过拟合产生的原因：模型太过于复杂、数据不纯、训练数据太小
- 泛化 Generalization：模型在新数据集（非训练数据）上的表现好坏的能力
- 奥卡姆剃刀原则：给定两个具有相同泛化误差的模型，较简单的模型比更复杂的模型更可取（说人话：大道至简）

