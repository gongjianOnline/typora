# 02. KNN近邻算法

属于机器学习中的有监督学习中的（分类和回归）

解决：分类和回归问题

### 算法思想

若一个样本在特征空间中的K个最相似的样本，大多数属于某一个类别，则该样本也属于这个类别

### 相似性

欧式距离

### K值的选择

K值过小：相当于较小领域中的训练实例进行预测，容易受到异常点的影响

举例：K=N （N为训练样本个数）

无论输入实例是啥，只会按训练集中最多类别进行预测，收样本均衡的影响

K值过大：相当于用较大领域中训练实例进行预测，受到样本均衡的问题

且 k 值的增大就意味着整体模型变得简单，欠拟合

如何对 K 超参数进行调优？ 交叉验证 网格搜索

### 分类问题处理流程

1. 计算位置样本到每个训练样本的距离
2. 将训练样本根据距离大小升序
3. 取出距离最近的K个训练样本
4. 进行多数表决，统计 K 个样本中哪个类别的样本个数最高
5. 将未知的样本归属到出现次数最多的类别

### 回归问题处理流程

1. 计算位置样本到每个训练样本的距离
2. 将训练样本根据距离大小升序
3. 去除距离最近K个训练样本
4. 把K个样本的目标值计算平均
5. 将未知的样本归属到出现次数最多的类别

### API实例

分类

```python
sklearn.neighbors.KneighbrsClassifier(n_neighbors=5)
```

回归

```python
sklearn.neighbors.KneighbrsRegressor(n_neighbors=5)
```

---

## 特征预处理

### 归一化&标准化

特征的单位或者大小相差太大，或者特征的方差相比其他的特征要大出几个数量级，容易影响到目标结果，是的一些模型无法学习到其他特征

### 归一化

通过对原始数据进行变换把数据映射到 [mix,min] 之间（0，1）

#### API

```python
sklean.preprocessing.MinMaxScaler(feature_range=(0,1),...)
```

注：

归一化受到最大值与最小值的影响，这种方法容易受到异常数据的影响，鲁棒性较差，适合传统精确小数据场景

### 标准化

通过对原始数据进行标准化，转为均值为 0 的标准差为1的标准正态分布数据

#### API 

```python
sklearn.preprocessing.standardScaler()
```

调用 fit_transform(x) 将特征进行归一化缩放

---

## 超参数选择方法（Cross Validation）

交叉验证 & 网络搜索

### 交叉验证

是一种数据集的分割方法，将训练集划分为 n 份，那一份做测试集，其他 n-1 做训练集

原理：将数据集分为 CV=4 份

1. 把第一份数据做测试，其他训练
2. 把第二份数据做测试，其他训练
3. 。。。以此类推，总共训练四次，评估四次
4. 使用训练集 + 测试机 多次评估模型，取平均做交叉验证为模型得分
5. 若 K = 5 模型得分好，在使用全部数据集（训练+测试）对 K=5 模型在练一遍，在使用测试集对k=5模型做评估

交叉验证，是划分数据集的一种方法，目的就是为了得到更加准确可信的模型评分

**CV：确定数据集分割得到最优方式**

### 网格搜索

模型调参的工具，寻找最优超参数的工具

是一种需要若干参数传递给网格搜索对象，他自动帮我们完成不同参数的组合，模型训练，模型评估，最后返回一组最优的超参数

**网格搜索 + 交叉验证的强力组合方式（模型选择和调优）**

- 交叉验证解决模型的数据集输入问题（数据集划分）得到更可靠的模型
- 网格搜索解决超参数的组合，两个组合到一起形成一个模型参数调优的解决方案

### API应用

```python
sklearn.model_selection.GridSearchcv(estimator,param_grid=NONE,cv=None)
```

- estimator：估计器对象
- param_grid：估计器参数（dict）{'n_neighbors':[1,3,5,7]}
- cv：指定几折交叉验证

- 输出
- fit 输出训练数据
- score 准确率
- 分析结果
  - best_score_：在交叉验证中验证最好的结果得分
  - best_estimator_：最好的参数模型
  - best_result_：每次交叉验证集准确结果和训练集准确率结果
  - best_params：最好的模型参数

